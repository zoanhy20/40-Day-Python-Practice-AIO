{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvp5AogX4y2BY90HAagPm1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoanhy20/40-Day-Python-Practice-AIO/blob/main/40_Day_Python_Practice_Day_37.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gLALmXQXBnR",
        "outputId": "2cef7dec-0594-4b6e-effa-5db648b468ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qq pypdf\n",
        "!pip install -qq arxiv-downloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!arxiv-downloader 2402.13616"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj5ckhGSZ_c0",
        "outputId": "c79c8efd-cd84-4bd2-ec9e-4f7d899e58d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting download of article: \"YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information\" (2402.13616)\n",
            "Download finished! Result saved at:\n",
            "./2402.13616v2.YOLOv9__Learning_What_You_Want_to_Learn_Using_Programmable_Gradient_Information.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 2402.13616v2.YOLOv9__Learning_What_You_Want_to_Learn_Using_Programmable_Gradient_Information.pdf yolov9.pdf"
      ],
      "metadata": {
        "id": "hiY4lyi1aMs5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trích xuất văn bản từ pdf"
      ],
      "metadata": {
        "id": "-SKFH55-cDwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "reader = PdfReader('yolov9.pdf')"
      ],
      "metadata": {
        "id": "7r3-3x2dbrrA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_pages = len(reader.pages)\n",
        "num_pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3DuXWTJbs8w",
        "outputId": "e5f3fa18-a2b1-4f10-9689-fbf36559bd9c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page_1 = reader.pages[0]\n",
        "page_1_txt = page_1.extract_text()\n",
        "page_1_txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "SAuJBwgja8jM",
        "outputId": "3e2a74bd-e968-4499-b16e-f980ef33788d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'YOLOv9: Learning What You Want to Learn\\nUsing Programmable Gradient Information\\nChien-Yao Wang1,2, I-Hau Yeh2, and Hong-Yuan Mark Liao1,2,3\\n1Institute of Information Science, Academia Sinica, Taiwan\\n2National Taipei University of Technology, Taiwan\\n3Department of Information and Computer Engineering, Chung Yuan Christian University, Taiwan\\nkinyiu@iis.sinica.edu.tw, ihyeh@emc.com.tw, and liao@iis.sinica.edu.tw\\nAbstract\\nToday’s deep learning methods focus on how to design\\nthe most appropriate objective functions so that the pre-\\ndiction results of the model can be closest to the ground\\ntruth. Meanwhile, an appropriate architecture that can\\nfacilitate acquisition of enough information for prediction\\nhas to be designed. Existing methods ignore a fact that\\nwhen input data undergoes layer-by-layer feature extrac-\\ntion and spatial transformation, large amount of informa-\\ntion will be lost. This paper will delve into the important is-\\nsues of data loss when data is transmitted through deep net-\\nworks, namely information bottleneck and reversible func-\\ntions. We proposed the concept of programmable gradi-\\nent information (PGI) to cope with the various changes\\nrequired by deep networks to achieve multiple objectives.\\nPGI can provide complete input information for the tar-\\nget task to calculate objective function, so that reliable\\ngradient information can be obtained to update network\\nweights. In addition, a new lightweight network architec-\\nture – Generalized Efficient Layer Aggregation Network\\n(GELAN), based on gradient path planning is designed.\\nGELAN’s architecture confirms that PGI has gained su-\\nperior results on lightweight models. We verified the pro-\\nposed GELAN and PGI on MS COCO dataset based ob-\\nject detection. The results show that GELAN only uses\\nconventional convolution operators to achieve better pa-\\nrameter utilization than the state-of-the-art methods devel-\\noped based on depth-wise convolution. PGI can be used\\nfor variety of models from lightweight to large. It can be\\nused to obtain complete information, so that train-from-\\nscratch models can achieve better results than state-of-the-\\nart models pre-trained using large datasets, the compari-\\nson results are shown in Figure 1. The source codes are at:\\nhttps://github.com/WongKinYiu/yolov9 .\\n1. Introduction\\nDeep learning-based models have demonstrated far bet-\\nter performance than past artificial intelligence systems in\\nvarious fields, such as computer vision, language process-\\ning, and speech recognition. In recent years, researchers\\nFigure 1. Comparisons of the real-time object detecors on MS\\nCOCO dataset. The GELAN and PGI-based object detection\\nmethod surpassed all previous train-from-scratch methods in terms\\nof object detection performance. In terms of accuracy, the new\\nmethod outperforms RT DETR [43] pre-trained with a large\\ndataset, and it also outperforms depth-wise convolution-based de-\\nsign YOLO MS [7] in terms of parameters utilization.\\nin the field of deep learning have mainly focused on how\\nto develop more powerful system architectures and learn-\\ning methods, such as CNNs [21–23, 42, 55, 71, 72], Trans-\\nformers [8, 9, 40, 41, 60, 69, 70], Perceivers [26, 26, 32, 52,\\n56, 81, 81], and Mambas [17, 38, 80]. In addition, some\\nresearchers have tried to develop more general objective\\nfunctions, such as loss function [5, 45, 46, 50, 77, 78], la-\\nbel assignment [10, 12, 33, 67, 79] and auxiliary supervi-\\nsion [18, 20, 24, 28, 29, 51, 54, 68, 76]. The above studies\\nall try to precisely find the mapping between input and tar-\\nget tasks. However, most past approaches have ignored that\\ninput data may have a non-negligible amount of informa-\\ntion loss during the feedforward process. This loss of in-\\nformation can lead to biased gradient flows, which are sub-\\nsequently used to update the model. The above problems\\ncan result in deep networks to establish incorrect associa-\\ntions between targets and inputs, causing the trained model\\nto produce incorrect predictions.\\n1arXiv:2402.13616v2  [cs.CV]  29 Feb 2024'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pages_txt = ''\n",
        "\n",
        "for i in range(num_pages):\n",
        "    page = reader.pages[i]\n",
        "    pages_txt += page.extract_text() + '\\n'"
      ],
      "metadata": {
        "id": "QPlQAL-Sbxef"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages_txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "TD5ib8QWb9u1",
        "outputId": "54bc2ec8-70a1-488a-b35e-a89f9b563961"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'YOLOv9: Learning What You Want to Learn\\nUsing Programmable Gradient Information\\nChien-Yao Wang1,2, I-Hau Yeh2, and Hong-Yuan Mark Liao1,2,3\\n1Institute of Information Science, Academia Sinica, Taiwan\\n2National Taipei University of Technology, Taiwan\\n3Department of Information and Computer Engineering, Chung Yuan Christian University, Taiwan\\nkinyiu@iis.sinica.edu.tw, ihyeh@emc.com.tw, and liao@iis.sinica.edu.tw\\nAbstract\\nToday’s deep learning methods focus on how to design\\nthe most appropriate objective functions so that the pre-\\ndiction results of the model can be closest to the ground\\ntruth. Meanwhile, an appropriate architecture that can\\nfacilitate acquisition of enough information for prediction\\nhas to be designed. Existing methods ignore a fact that\\nwhen input data undergoes layer-by-layer feature extrac-\\ntion and spatial transformation, large amount of informa-\\ntion will be lost. This paper will delve into the important is-\\nsues of data loss when data is transmitted through deep net-\\nworks, namely information bottleneck and reversible func-\\ntions. We proposed the concept of programmable gradi-\\nent information (PGI) to cope with the various changes\\nrequired by deep networks to achieve multiple objectives.\\nPGI can provide complete input information for the tar-\\nget task to calculate objective function, so that reliable\\ngradient information can be obtained to update network\\nweights. In addition, a new lightweight network architec-\\nture – Generalized Efficient Layer Aggregation Network\\n(GELAN), based on gradient path planning is designed.\\nGELAN’s architecture confirms that PGI has gained su-\\nperior results on lightweight models. We verified the pro-\\nposed GELAN and PGI on MS COCO dataset based ob-\\nject detection. The results show that GELAN only uses\\nconventional convolution operators to achieve better pa-\\nrameter utilization than the state-of-the-art methods devel-\\noped based on depth-wise convolution. PGI can be used\\nfor variety of models from lightweight to large. It can be\\nused to obtain complete information, so that train-from-\\nscratch models can achieve better results than state-of-the-\\nart models pre-trained using large datasets, the compari-\\nson results are shown in Figure 1. The source codes are at:\\nhttps://github.com/WongKinYiu/yolov9 .\\n1. Introduction\\nDeep learning-based models have demonstrated far bet-\\nter performance than past artificial intelligence systems in\\nvarious fields, such as computer vision, language process-\\ning, and speech recognition. In recent years, researchers\\nFigure 1. Comparisons of the real-time object detecors on MS\\nCOCO dataset. The GELAN and PGI-based object detection\\nmethod surpassed all previous train-from-scratch methods in terms\\nof object detection performance. In terms of accuracy, the new\\nmethod outperforms RT DETR [43] pre-trained with a large\\ndataset, and it also outperforms depth-wise convolution-based de-\\nsign YOLO MS [7] in terms of parameters utilization.\\nin the field of deep learning have mainly focused on how\\nto develop more powerful system architectures and learn-\\ning methods, such as CNNs [21–23, 42, 55, 71, 72], Trans-\\nformers [8, 9, 40, 41, 60, 69, 70], Perceivers [26, 26, 32, 52,\\n56, 81, 81], and Mambas [17, 38, 80]. In addition, some\\nresearchers have tried to develop more general objective\\nfunctions, such as loss function [5, 45, 46, 50, 77, 78], la-\\nbel assignment [10, 12, 33, 67, 79] and auxiliary supervi-\\nsion [18, 20, 24, 28, 29, 51, 54, 68, 76]. The above studies\\nall try to precisely find the mapping between input and tar-\\nget tasks. However, most past approaches have ignored that\\ninput data may have a non-negligible amount of informa-\\ntion loss during the feedforward process. This loss of in-\\nformation can lead to biased gradient flows, which are sub-\\nsequently used to update the model. The above problems\\ncan result in deep networks to establish incorrect associa-\\ntions between targets and inputs, causing the trained model\\nto produce incorrect predictions.\\n1arXiv:2402.13616v2  [cs.CV]  29 Feb 2024\\nFigure 2. Visualization results of random initial weight output feature maps for different network architectures: (a) input image, (b)\\nPlainNet, (c) ResNet, (d) CSPNet, and (e) proposed GELAN. From the figure, we can see that in different architectures, the information\\nprovided to the objective function to calculate the loss is lost to varying degrees, and our architecture can retain the most complete\\ninformation and provide the most reliable gradient information for calculating the objective function.\\nIn deep networks, the phenomenon of input data losing\\ninformation during the feedforward process is commonly\\nknown as information bottleneck [59], and its schematic di-\\nagram is as shown in Figure 2. At present, the main meth-\\nods that can alleviate this phenomenon are as follows: (1)\\nThe use of reversible architectures [3, 16, 19]: this method\\nmainly uses repeated input data and maintains the informa-\\ntion of the input data in an explicit way; (2) The use of\\nmasked modeling [1, 6, 9, 27, 71, 73]: it mainly uses recon-\\nstruction loss and adopts an implicit way to maximize the\\nextracted features and retain the input information; and (3)\\nIntroduction of the deep supervision concept [28,51,54,68]:\\nit uses shallow features that have not lost too much impor-\\ntant information to pre-establish a mapping from features\\nto targets to ensure that important information can be trans-\\nferred to deeper layers. However, the above methods have\\ndifferent drawbacks in the training process and inference\\nprocess. For example, a reversible architecture requires ad-\\nditional layers to combine repeatedly fed input data, which\\nwill significantly increase the inference cost. In addition,\\nsince the input data layer to the output layer cannot have a\\ntoo deep path, this limitation will make it difficult to model\\nhigh-order semantic information during the training pro-\\ncess. As for masked modeling, its reconstruction loss some-\\ntimes conflicts with the target loss. In addition, most mask\\nmechanisms also produce incorrect associations with data.\\nFor the deep supervision mechanism, it will produce error\\naccumulation, and if the shallow supervision loses informa-\\ntion during the training process, the subsequent layers will\\nnot be able to retrieve the required information. The above\\nphenomenon will be more significant on difficult tasks and\\nsmall models.\\nTo address the above-mentioned issues, we propose a\\nnew concept, which is programmable gradient information\\n(PGI). The concept is to generate reliable gradients through\\nauxiliary reversible branch, so that the deep features can\\nstill maintain key characteristics for executing target task.\\nThe design of auxiliary reversible branch can avoid the se-\\nmantic loss that may be caused by a traditional deep super-\\nvision process that integrates multi-path features. In other\\nwords, we are programming gradient information propaga-\\ntion at different semantic levels, and thereby achieving the\\nbest training results. The reversible architecture of PGI isbuilt on auxiliary branch, so there is no additional cost.\\nSince PGI can freely select loss function suitable for the\\ntarget task, it also overcomes the problems encountered by\\nmask modeling. The proposed PGI mechanism can be ap-\\nplied to deep neural networks of various sizes and is more\\ngeneral than the deep supervision mechanism, which is only\\nsuitable for very deep neural networks.\\nIn this paper, we also designed generalized ELAN\\n(GELAN) based on ELAN [65], the design of GELAN si-\\nmultaneously takes into account the number of parameters,\\ncomputational complexity, accuracy and inference speed.\\nThis design allows users to arbitrarily choose appropriate\\ncomputational blocks for different inference devices. We\\ncombined the proposed PGI and GELAN, and then de-\\nsigned a new generation of YOLO series object detection\\nsystem, which we call YOLOv9. We used the MS COCO\\ndataset to conduct experiments, and the experimental results\\nverified that our proposed YOLOv9 achieved the top perfor-\\nmance in all comparisons.\\nWe summarize the contributions of this paper as follows:\\n1. We theoretically analyzed the existing deep neural net-\\nwork architecture from the perspective of reversible\\nfunction, and through this process we successfully ex-\\nplained many phenomena that were difficult to explain\\nin the past. We also designed PGI and auxiliary re-\\nversible branch based on this analysis and achieved ex-\\ncellent results.\\n2. The PGI we designed solves the problem that deep su-\\npervision can only be used for extremely deep neu-\\nral network architectures, and therefore allows new\\nlightweight architectures to be truly applied in daily\\nlife.\\n3. The GELAN we designed only uses conventional con-\\nvolution to achieve a higher parameter usage than the\\ndepth-wise convolution design that based on the most\\nadvanced technology, while showing great advantages\\nof being light, fast, and accurate.\\n4. Combining the proposed PGI and GELAN, the object\\ndetection performance of the YOLOv9 on MS COCO\\ndataset greatly surpasses the existing real-time object\\ndetectors in all aspects.\\n2\\n2. Related work\\n2.1. Real-time Object Detectors\\nThe current mainstream real-time object detectors are the\\nYOLO series [2, 7, 13–15, 25, 30, 31, 47–49, 61–63, 74, 75],\\nand most of these models use CSPNet [64] or ELAN [65]\\nand their variants as the main computing units. In terms of\\nfeature integration, improved PAN [37] or FPN [35] is of-\\nten used as a tool, and then improved YOLOv3 head [49] or\\nFCOS head [57, 58] is used as prediction head. Recently\\nsome real-time object detectors, such as RT DETR [43],\\nwhich puts its fundation on DETR [4], have also been pro-\\nposed. However, since it is extremely difficult for DETR\\nseries object detector to be applied to new domains without\\na corresponding domain pre-trained model, the most widely\\nused real-time object detector at present is still YOLO se-\\nries. This paper chooses YOLOv7 [63], which has been\\nproven effective in a variety of computer vision tasks and\\nvarious scenarios, as a base to develop the proposed method.\\nWe use GELAN to improve the architecture and the training\\nprocess with the proposed PGI. The above novel approach\\nmakes the proposed YOLOv9 the top real-time object de-\\ntector of the new generation.\\n2.2. Reversible Architectures\\nThe operation unit of reversible architectures [3, 16, 19]\\nmust maintain the characteristics of reversible conversion,\\nso it can be ensured that the output feature map of each\\nlayer of operation unit can retain complete original informa-\\ntion. Before, RevCol [3] generalizes traditional reversible\\nunit to multiple levels, and in doing so can expand the se-\\nmantic levels expressed by different layer units. Through\\na literature review of various neural network architectures,\\nwe found that there are many high-performing architectures\\nwith varying degree of reversible properties. For exam-\\nple, Res2Net module [11] combines different input parti-\\ntions with the next partition in a hierarchical manner, and\\nconcatenates all converted partitions before passing them\\nbackwards. CBNet [34, 39] re-introduces the original in-\\nput data through composite backbone to obtain complete\\noriginal information, and obtains different levels of multi-\\nlevel reversible information through various composition\\nmethods. These network architectures generally have ex-\\ncellent parameter utilization, but the extra composite layers\\ncause slow inference speeds. DynamicDet [36] combines\\nCBNet [34] and the high-efficiency real-time object detec-\\ntor YOLOv7 [63] to achieve a very good trade-off among\\nspeed, number of parameters, and accuracy. This paper in-\\ntroduces the DynamicDet architecture as the basis for de-\\nsigning reversible branches. In addition, reversible infor-\\nmation is further introduced into the proposed PGI. The\\nproposed new architecture does not require additional con-\\nnections during the inference process, so it can fully retain\\nthe advantages of speed, parameter amount, and accuracy.2.3. Auxiliary Supervision\\nDeep supervision [28,54,68] is the most common auxil-\\niary supervision method, which performs training by insert-\\ning additional prediction layers in the middle layers. Es-\\npecially the application of multi-layer decoders introduced\\nin the transformer-based methods is the most common one.\\nAnother common auxiliary supervision method is to utilize\\nthe relevant meta information to guide the feature maps pro-\\nduced by the intermediate layers and make them have the\\nproperties required by the target tasks [18, 20, 24, 29, 76].\\nExamples of this type include using segmentation loss or\\ndepth loss to enhance the accuracy of object detectors. Re-\\ncently, there are many reports in the literature [53, 67, 82]\\nthat use different label assignment methods to generate dif-\\nferent auxiliary supervision mechanisms to speed up the\\nconvergence speed of the model and improve the robustness\\nat the same time. However, the auxiliary supervision mech-\\nanism is usually only applicable to large models, so when\\nit is applied to lightweight models, it is easy to cause an\\nunder parameterization phenomenon, which makes the per-\\nformance worse. The PGI we proposed designed a way to\\nreprogram multi-level semantic information, and this design\\nallows lightweight models to also benefit from the auxiliary\\nsupervision mechanism.\\n3. Problem Statement\\nUsually, people attribute the difficulty of deep neural net-\\nwork convergence problem due to factors such as gradient\\nvanish or gradient saturation, and these phenomena do ex-\\nist in traditional deep neural networks. However, modern\\ndeep neural networks have already fundamentally solved\\nthe above problem by designing various normalization and\\nactivation functions. Nevertheless, deep neural networks\\nstill have the problem of slow convergence or poor conver-\\ngence results.\\nIn this paper, we explore the nature of the above issue\\nfurther. Through in-depth analysis of information bottle-\\nneck, we deduced that the root cause of this problem is that\\nthe initial gradient originally coming from a very deep net-\\nwork has lost a lot of information needed to achieve the\\ngoal soon after it is transmitted. In order to confirm this\\ninference, we feedforward deep networks of different archi-\\ntectures with initial weights, and then visualize and illus-\\ntrate them in Figure 2. Obviously, PlainNet has lost a lot of\\nimportant information required for object detection in deep\\nlayers. As for the proportion of important information that\\nResNet, CSPNet, and GELAN can retain, it is indeed posi-\\ntively related to the accuracy that can be obtained after train-\\ning. We further design reversible network-based methods to\\nsolve the causes of the above problems. In this section we\\nshall elaborate our analysis of information bottleneck prin-\\nciple and reversible functions.\\n3\\n3.1. Information Bottleneck Principle\\nAccording to information bottleneck principle, we know\\nthat data Xmay cause information loss when going through\\ntransformation, as shown in Eq. 1 below:\\nI(X, X )≥I(X, f θ(X))≥I(X, gϕ(fθ(X))),(1)\\nwhere Iindicates mutual information, fandgare transfor-\\nmation functions, and θandϕare parameters of fandg,\\nrespectively.\\nIn deep neural networks, fθ(·)andgϕ(·)respectively\\nrepresent the operations of two consecutive layers in deep\\nneural network. From Eq. 1, we can predict that as the num-\\nber of network layer becomes deeper, the original data will\\nbe more likely to be lost. However, the parameters of the\\ndeep neural network are based on the output of the network\\nas well as the given target, and then update the network after\\ngenerating new gradients by calculating the loss function.\\nAs one can imagine, the output of a deeper neural network\\nis less able to retain complete information about the pre-\\ndiction target. This will make it possible to use incomplete\\ninformation during network training, resulting in unreliable\\ngradients and poor convergence.\\nOne way to solve the above problem is to directly in-\\ncrease the size of the model. When we use a large number\\nof parameters to construct a model, it is more capable of\\nperforming a more complete transformation of the data. The\\nabove approach allows even if information is lost during the\\ndata feedforward process, there is still a chance to retain\\nenough information to perform the mapping to the target.\\nThe above phenomenon explains why the width is more im-\\nportant than the depth in most modern models. However,\\nthe above conclusion cannot fundamentally solve the prob-\\nlem of unreliable gradients in very deep neural network.\\nBelow, we will introduce how to use reversible functions\\nto solve problems and conduct relative analysis.\\n3.2. Reversible Functions\\nWhen a function rhas an inverse transformation func-\\ntionv, we call this function reversible function, as shown in\\nEq. 2.\\nX=vζ(rψ(X)), (2)\\nwhere ψandζare parameters of randv, respectively. Data\\nXis converted by reversible function without losing infor-\\nmation, as shown in Eq. 3.\\nI(X, X ) =I(X, rψ(X)) =I(X, v ζ(rψ(X))).(3)\\nWhen the network’s transformation function is composed\\nof reversible functions, more reliable gradients can be ob-\\ntained to update the model. Almost all of today’s populardeep learning methods are architectures that conform to the\\nreversible property, such as Eq. 4.\\nXl+1=Xl+fl+1\\nθ(Xl), (4)\\nwhere lindicates the l-th layer of a PreAct ResNet and\\nfis the transformation function of the l-th layer. PreAct\\nResNet [22] repeatedly passes the original data Xto sub-\\nsequent layers in an explicit way. Although such a design\\ncan make a deep neural network with more than a thousand\\nlayers converge very well, it destroys an important reason\\nwhy we need deep neural networks. That is, for difficult\\nproblems, it is difficult for us to directly find simple map-\\nping functions to map data to targets. This also explains\\nwhy PreAct ResNet performs worse than ResNet [21] when\\nthe number of layers is small.\\nIn addition, we tried to use masked modeling that al-\\nlowed the transformer model to achieve significant break-\\nthroughs. We use approximation methods, such as Eq. 5,\\nto try to find the inverse transformation vofr, so that the\\ntransformed features can retain enough information using\\nsparse features. The form of Eq. 5 is as follows:\\nX=vζ(rψ(X)·M), (5)\\nwhere Mis a dynamic binary mask. Other methods that\\nare commonly used to perform the above tasks are diffusion\\nmodel and variational autoencoder, and they both have the\\nfunction of finding the inverse function. However, when\\nwe apply the above approach to a lightweight model, there\\nwill be defects because the lightweight model will be under\\nparameterized to a large amount of raw data. Because of\\nthe above reason, important information I(Y, X)that maps\\ndataXto target Ywill also face the same problem. For this\\nissue, we will explore it using the concept of information\\nbottleneck [59]. The formula for information bottleneck is\\nas follows:\\nI(X, X )≥I(Y, X)≥I(Y, fθ(X))≥...≥I(Y,ˆY).(6)\\nGenerally speaking, I(Y, X)will only occupy a very small\\npart of I(X, X ). However, it is critical to the target mis-\\nsion. Therefore, even if the amount of information lost in\\nthe feedforward stage is not significant, as long as I(Y, X)\\nis covered, the training effect will be greatly affected. The\\nlightweight model itself is in an under parameterized state,\\nso it is easy to lose a lot of important information in the\\nfeedforward stage. Therefore, our goal for the lightweight\\nmodel is how to accurately filter I(Y, X)fromI(X, X ). As\\nfor fully preserving the information of X, that is difficult to\\nachieve. Based on the above analysis, we hope to propose a\\nnew deep neural network training method that can not only\\ngenerate reliable gradients to update the model, but also be\\nsuitable for shallow and lightweight neural networks.\\n4\\nFigure 3. PGI and related network architectures and methods. (a) Path Aggregation Network (PAN)) [37], (b) Reversible Columns\\n(RevCol) [3], (c) conventional deep supervision, and (d) our proposed Programmable Gradient Information (PGI). PGI is mainly composed\\nof three components: (1) main branch: architecture used for inference, (2) auxiliary reversible branch: generate reliable gradients to supply\\nmain branch for backward transmission, and (3) multi-level auxiliary information: control main branch learning plannable multi-level of\\nsemantic information.\\n4. Methodology\\n4.1. Programmable Gradient Information\\nIn order to solve the aforementioned problems, we pro-\\npose a new auxiliary supervision framework called Pro-\\ngrammable Gradient Information (PGI), as shown in Fig-\\nure 3 (d). PGI mainly includes three components, namely\\n(1) main branch, (2) auxiliary reversible branch, and (3)\\nmulti-level auxiliary information. From Figure 3 (d) we\\nsee that the inference process of PGI only uses main branch\\nand therefore does not require any additional inference cost.\\nAs for the other two components, they are used to solve or\\nslow down several important issues in deep learning meth-\\nods. Among them, auxiliary reversible branch is designed\\nto deal with the problems caused by the deepening of neural\\nnetworks. Network deepening will cause information bot-\\ntleneck, which will make the loss function unable to gener-\\nate reliable gradients. As for multi-level auxiliary informa-\\ntion, it is designed to handle the error accumulation problem\\ncaused by deep supervision, especially for the architecture\\nand lightweight model of multiple prediction branch. Next,\\nwe will introduce these two components step by step.\\n4.1.1 Auxiliary Reversible Branch\\nIn PGI, we propose auxiliary reversible branch to gener-\\nate reliable gradients and update network parameters. By\\nproviding information that maps from data to targets, the\\nloss function can provide guidance and avoid the possibil-\\nity of finding false correlations from incomplete feedfor-\\nward features that are less relevant to the target. We pro-pose the maintenance of complete information by introduc-\\ning reversible architecture, but adding main branch to re-\\nversible architecture will consume a lot of inference costs.\\nWe analyzed the architecture of Figure 3 (b) and found that\\nwhen additional connections from deep to shallow layers\\nare added, the inference time will increase by 20%. When\\nwe repeatedly add the input data to the high-resolution com-\\nputing layer of the network (yellow box), the inference time\\neven exceeds twice the time.\\nSince our goal is to use reversible architecture to ob-\\ntain reliable gradients, “reversible” is not the only neces-\\nsary condition in the inference stage. In view of this, we\\nregard reversible branch as an expansion of deep supervi-\\nsion branch, and then design auxiliary reversible branch, as\\nshown in Figure 3 (d). As for the main branch deep fea-\\ntures that would have lost important information due to in-\\nformation bottleneck, they will be able to receive reliable\\ngradient information from the auxiliary reversible branch.\\nThese gradient information will drive parameter learning to\\nassist in extracting correct and important information, and\\nthe above actions can enable the main branch to obtain fea-\\ntures that are more effective for the target task. Moreover,\\nthe reversible architecture performs worse on shallow net-\\nworks than on general networks because complex tasks re-\\nquire conversion in deeper networks. Our proposed method\\ndoes not force the main branch to retain complete origi-\\nnal information but updates it by generating useful gradient\\nthrough the auxiliary supervision mechanism. The advan-\\ntage of this design is that the proposed method can also be\\napplied to shallower networks.\\n5\\nFigure 4. The architecture of GELAN: (a) CSPNet [64], (b) ELAN [65], and (c) proposed GELAN. We imitate CSPNet and extend ELAN\\ninto GELAN that can support any computational blocks.\\nFinally, since auxiliary reversible branch can be removed\\nduring the inference phase, the inference capabilities of the\\noriginal network can be retained. We can also choose any\\nreversible architectures in PGI to play the role of auxiliary\\nreversible branch.\\n4.1.2 Multi-level Auxiliary Information\\nIn this section we will discuss how multi-level auxiliary in-\\nformation works. The deep supervision architecture includ-\\ning multiple prediction branch is shown in Figure 3 (c). For\\nobject detection, different feature pyramids can be used to\\nperform different tasks, for example together they can de-\\ntect objects of different sizes. Therefore, after connecting\\nto the deep supervision branch, the shallow features will be\\nguided to learn the features required for small object detec-\\ntion, and at this time the system will regard the positions\\nof objects of other sizes as the background. However, the\\nabove deed will cause the deep feature pyramids to lose a lot\\nof information needed to predict the target object. Regard-\\ning this issue, we believe that each feature pyramid needs\\nto receive information about all target objects so that subse-\\nquent main branch can retain complete information to learn\\npredictions for various targets.\\nThe concept of multi-level auxiliary information is to in-\\nsert an integration network between the feature pyramid hi-\\nerarchy layers of auxiliary supervision and the main branch,\\nand then uses it to combine returned gradients from differ-\\nent prediction heads, as shown in Figure 3 (d). Multi-level\\nauxiliary information is then to aggregate the gradient infor-\\nmation containing all target objects, and pass it to the main\\nbranch and then update parameters. At this time, the charac-\\nteristics of the main branch’s feature pyramid hierarchy will\\nnot be dominated by some specific object’s information. As\\na result, our method can alleviate the broken information\\nproblem in deep supervision. In addition, any integrated\\nnetwork can be used in multi-level auxiliary information.\\nTherefore, we can plan the required semantic levels to guide\\nthe learning of network architectures of different sizes.4.2. Generalized ELAN\\nIn this Section we describe the proposed new network\\narchitecture – GELAN. By combining two neural network\\narchitectures, CSPNet [64] and ELAN [65], which are de-\\nsigned with gradient path planning, we designed gener-\\nalized efficient layer aggregation network (GELAN) that\\ntakes into account lighweight, inference speed, and accu-\\nracy. Its overall architecture is shown in Figure 4. We gen-\\neralized the capability of ELAN [65], which originally only\\nused stacking of convolutional layers, to a new architecture\\nthat can use any computational blocks.\\n5. Experiments\\n5.1. Experimental Setup\\nWe verify the proposed method with MS COCO dataset.\\nAll experimental setups follow YOLOv7 AF [63], while the\\ndataset is MS COCO 2017 splitting. All models we men-\\ntioned are trained using the train-from-scratch strategy, and\\nthe total number of training times is 500 epochs. In setting\\nthe learning rate, we use linear warm-up in the first three\\nepochs, and the subsequent epochs set the corresponding\\ndecay manner according to the model scale. As for the last\\n15 epochs, we turn mosaic data augmentation off. For more\\nsettings, please refer to Appendix.\\n5.2. Implimentation Details\\nWe built general and extended version of YOLOv9 based\\non YOLOv7 [63] and Dynamic YOLOv7 [36] respectively.\\nIn the design of the network architecture, we replaced\\nELAN [65] with GELAN using CSPNet blocks [64] with\\nplanned RepConv [63] as computational blocks. We also\\nsimplified downsampling module and optimized anchor-\\nfree prediction head. As for the auxiliary loss part of PGI,\\nwe completely follow YOLOv7’s auxiliary head setting.\\nPlease see Appendix for more details.\\n6\\nTable 1. Comparison of state-of-the-art real-time object detectors.\\nModel #Param. (M) FLOPs (G) APval\\n50:95(%) APval\\n50(%) APval\\n75(%) APval\\nS(%) APval\\nM(%) APval\\nL(%)\\nYOLOv5-N r7.0 [14] 1.9 4.5 28.0 45.7 – – – –\\nYOLOv5-S r7.0 [14] 7.2 16.5 37.4 56.8 – – – –\\nYOLOv5-M r7.0 [14] 21.2 49.0 45.4 64.1 – – – –\\nYOLOv5-L r7.0 [14] 46.5 109.1 49.0 67.3 – – – –\\nYOLOv5-X r7.0 [14] 86.7 205.7 50.7 68.9 – – – –\\nYOLOv6-N v3.0 [30] 4.7 11.4 37.0 52.7 – – – –\\nYOLOv6-S v3.0 [30] 18.5 45.3 44.3 61.2 – – – –\\nYOLOv6-M v3.0 [30] 34.9 85.8 49.1 66.1 – – – –\\nYOLOv6-L v3.0 [30] 59.6 150.7 51.8 69.2 – – – –\\nYOLOv7 [63] 36.9 104.7 51.2 69.7 55.9 31.8 55.5 65.0\\nYOLOv7-X [63] 71.3 189.9 52.9 71.1 51.4 36.9 57.7 68.6\\nYOLOv7-N AF [63] 3.1 8.7 37.6 53.3 40.6 18.7 41.7 52.8\\nYOLOv7-S AF [63] 11.0 28.1 45.1 61.8 48.9 25.7 50.2 61.2\\nYOLOv7 AF [63] 43.6 130.5 53.0 70.2 57.5 35.8 58.7 68.9\\nYOLOv8-N [15] 3.2 8.7 37.3 52.6 – – – –\\nYOLOv8-S [15] 11.2 28.6 44.9 61.8 – – – –\\nYOLOv8-M [15] 25.9 78.9 50.2 67.2 – – – –\\nYOLOv8-L [15] 43.7 165.2 52.9 69.8 57.5 35.3 58.3 69.8\\nYOLOv8-X [15] 68.2 257.8 53.9 71.0 58.7 35.7 59.3 70.7\\nDAMO YOLO-T [75] 8.5 18.1 42.0 58.0 45.2 23.0 46.1 58.5\\nDAMO YOLO-S [75] 12.3 37.8 46.0 61.9 49.5 25.9 50.6 62.5\\nDAMO YOLO-M [75] 28.2 61.8 49.2 65.5 53.0 29.7 53.1 66.1\\nDAMO YOLO-L [75] 42.1 97.3 50.8 67.5 55.5 33.2 55.7 66.6\\nGold YOLO-N [61] 5.6 12.1 39.6 55.7 – 19.7 44.1 57.0\\nGold YOLO-S [61] 21.5 46.0 45.4 62.5 – 25.3 50.2 62.6\\nGold YOLO-M [61] 41.3 87.5 49.8 67.0 – 32.3 55.3 66.3\\nGold YOLO-L [61] 75.1 151.7 51.8 68.9 – 34.1 57.4 68.2\\nYOLO MS-N [7] 4.5 17.4 43.4 60.4 47.6 23.7 48.3 60.3\\nYOLO MS-S [7] 8.1 31.2 46.2 63.7 50.5 26.9 50.5 63.0\\nYOLO MS [7] 22.2 80.2 51.0 68.6 55.7 33.1 56.1 66.5\\nGELAN-S (Ours) 7.1 26.4 46.7 63.0 50.7 25.9 51.5 64.0\\nGELAN-M (Ours) 20.0 76.3 51.1 67.9 55.7 33.6 56.4 67.3\\nGELAN-C (Ours) 25.3 102.1 52.5 69.5 57.3 35.8 57.6 69.4\\nGELAN-E (Ours) 57.3 189.0 55.0 71.9 60.0 38.0 60.6 70.9\\nYOLOv9-S (Ours) 7.1 26.4 46.8 63.4 50.7 26.6 56.0 64.5\\nYOLOv9-M (Ours) 20.0 76.3 51.4 68.1 56.1 33.6 57.0 68.0\\nYOLOv9-C (Ours) 25.3 102.1 53.0 70.2 57.8 36.2 58.5 69.3\\nYOLOv9-E (Ours) 57.3 189.0 55.6 72.8 60.6 40.2 61.0 71.4\\n5.3. Comparison with state-of-the-arts\\nTable 1 lists comparison of our proposed YOLOv9 with\\nother train-from-scratch real-time object detectors. Over-\\nall, the best performing methods among existing methods\\nare YOLO MS-S [7] for lightweight models, YOLO MS [7]\\nfor medium models, YOLOv7 AF [63] for general mod-\\nels, and YOLOv8-X [15] for large models. Compared with\\nlightweight and medium model YOLO MS [7], YOLOv9\\nhas about 10% less parameters and 5 ∼15% less calcula-\\ntions, but still has a 0.4 ∼0.6% improvement in AP. Com-\\npared with YOLOv7 AF, YOLOv9-C has 42% less pa-\\nrameters and 22% less calculations, but achieves the same\\nAP (53%). Compared with YOLOv8-X, YOLOv9-E has\\n16% less parameters, 27% less calculations, and has sig-\\nnificant improvement of 1.7% AP. The above comparison\\nresults show that our proposed YOLOv9 has significantlyimproved in all aspects compared with existing methods.\\nOn the other hand, we also include ImageNet pretrained\\nmodel in the comparison, and the results are shown in Fig-\\nure 5. We compare them based on the parameters and the\\namount of computation respectively. In terms of the num-\\nber of parameters, the best performing large model is RT\\nDETR [43]. From Figure 5, we can see that YOLOv9 using\\nconventional convolution is even better than YOLO MS us-\\ning depth-wise convolution in parameter utilization. As for\\nthe parameter utilization of large models, it also greatly sur-\\npasses RT DETR using ImageNet pretrained model. Even\\nbetter is that in the deep model, YOLOv9 shows the huge\\nadvantages of using PGI. By accurately retaining and ex-\\ntracting the information needed to map the data to the tar-\\nget, our method requires only 66% of the parameters while\\nmaintaining the accuracy as RT DETR-X.\\n7\\nFigure 5. Comparison of state-of-the-art real-time object detectors. The methods participating in the comparison all use ImageNet as\\npre-trained weights, including RT DETR [43], RTMDet [44], and PP-YOLOE [74], etc. The YOLOv9 that uses train-from-scratch method\\nclearly surpasses the performance of other methods.\\nAs for the amount of computation, the best existing mod-\\nels from the smallest to the largest are YOLO MS [7], PP\\nYOLOE [74], and RT DETR [43]. From Figure 5, we can\\nsee that YOLOv9 is far superior to the train-from-scratch\\nmethods in terms of computational complexity. In addi-\\ntion, if compared with those based on depth-wise convo-\\nlution and ImageNet-based pretrained models, YOLOv9 is\\nalso very competitive.\\n5.4. Ablation Studies\\n5.4.1 Generalized ELAN\\nFor GELAN, we first do ablation studies for computational\\nblocks. We used Res blocks [21], Dark blocks [49], and\\nCSP blocks [64] to conduct experiments, respectively. Ta-\\nble 2 shows that after replacing convolutional layers in\\nELAN with different computational blocks, the system can\\nmaintain good performance. Users are indeed free to re-\\nplace computational blocks and use them on their respective\\ninference devices. Among different computational block re-\\nplacements, CSP blocks perform particularly well. They\\nnot only reduce the amount of parameters and computation,\\nbut also improve AP by 0.7%. Therefore, we choose CSP-\\nELAN as the component unit of GELAN in YOLOv9.\\nTable 2. Ablation study on various computational blocks.\\nModel CB type #Param. FLOPs APval\\n50:95\\nGELAN-S Conv 6.2M 23.5G 44.8%\\nGELAN-S Res [21] 5.4M 21.0G 44.3%\\nGELAN-S Dark [49] 5.7M 21.8G 44.5%\\nGELAN-S CSP [64] 5.9M 22.4G 45.5%\\n1CB type nedotes as computational block type.\\n2-S nedotes small size model.Next, we conduct ELAN block-depth and CSP block-\\ndepth experiments on GELAN of different sizes, and dis-\\nplay the results in Table 3. We can see that when the depth\\nof ELAN is increased from 1 to 2, the accuracy is signif-\\nicantly improved. But when the depth is greater than or\\nequal to 2, no matter it is improving the ELAN depth or the\\nCSP depth, the number of parameters, the amount of com-\\nputation, and the accuracy will always show a linear rela-\\ntionship. This means GELAN is not sensitive to the depth.\\nIn other words, users can arbitrarily combine the compo-\\nnents in GELAN to design the network architecture, and\\nhave a model with stable performance without special de-\\nsign. In Table 3, for YOLOv9- {S,M,C}, we set the pairing\\nof the ELAN depth and the CSP depth to {{2, 3},{2, 1},\\n{2, 1}}.\\nTable 3. Ablation study on ELAN and CSP depth.\\nModel D ELAN DCSP #Param. FLOPs APval\\n50:95\\nGELAN-S 2 1 5.9M 22.4G 45.5%\\nGELAN-S 2 2 6.5M 24.4G 46.0%\\nGELAN-S 3 1 7.1M 26.3G 46.5%\\nGELAN-S 2 3 7.1M 26.4G 46.7%\\nGELAN-M 2 1 20.0M 76.3G 51.1%\\nGELAN-M 2 2 22.2M 85.1G 51.7%\\nGELAN-M 3 1 24.3M 93.5G 51.8%\\nGELAN-M 2 3 24.4M 94.0G 52.3%\\nGELAN-C 1 1 18.9M 77.5G 50.7%\\nGELAN-C 2 1 25.3M 102.1G 52.5%\\nGELAN-C 2 2 28.6M 114.4G 53.0%\\nGELAN-C 3 1 31.7M 126.8G 53.2%\\nGELAN-C 2 3 31.9M 126.7G 53.3%\\n1DELAN andDCSP respectively nedotes depth of ELAN and CSP.\\n2-{S, M, C }indicate small, medium, and compact models.\\n8\\n5.4.2 Programmable Gradient Information\\nIn terms of PGI, we performed ablation studies on auxiliary\\nreversible branch and multi-level auxiliary information on\\nthe backbone and neck, respectively. We designed auxiliary\\nreversible branch ICN to use DHLC [34] linkage to obtain\\nmulti-level reversible information. As for multi-level aux-\\niliary information, we use FPN and PAN for ablation stud-\\nies and the role of PFH is equivalent to the traditional deep\\nsupervision. The results of all experiments are listed in Ta-\\nble 4. From Table 4, we can see that PFH is only effective in\\ndeep models, while our proposed PGI can improve accuracy\\nunder different combinations. Especially when using ICN,\\nwe get stable and better results. We also tried to apply the\\nlead-head guided assignment proposed in YOLOv7 [63] to\\nthe PGI’s auxiliary supervision, and achieved much better\\nperformance.\\nTable 4. Ablation study on PGI of backbone and neck.\\nModel G backbone Gneck APval\\n50:95APval\\nSAPval\\nMAPval\\nL\\nGELAN-C – – 52.5% 35.8% 57.6% 69.4%\\nGELAN-C PFH – 52.5% 35.3% 58.1% 68.9%\\nGELAN-C FPN – 52.6% 35.3% 58.1% 68.9%\\nGELAN-C – ICN 52.7% 35.3% 58.4% 68.9%\\nGELAN-C FPN ICN 52.8% 35.8% 58.2% 69.1%\\nGELAN-C ICN – 52.9% 35.2% 58.7% 68.6%\\nGELAN-C LHG-ICN – 53.0% 36.3% 58.5% 69.1%\\nGELAN-E – – 55.0% 38.0% 60.6% 70.9%\\nGELAN-E PFH – 55.3% 38.3% 60.3% 71.6%\\nGELAN-E FPN – 55.6% 40.2% 61.0% 71.4%\\nGELAN-E PAN – 55.5% 39.0% 61.1% 71.5%\\nGELAN-E FPN ICN 55.6% 39.8% 60.9% 71.9%\\n1DELAN andDCSP respectively nedotes depth of ELAN and CSP.\\n2LHG indicates lead head guided training proposed by YOLOv7 [63].\\nWe further implemented the concepts of PGI and deep\\nsupervision on models of various sizes and compared the\\nresults, these results are shown in Table 5. As analyzed at\\nthe beginning, introduction of deep supervision will cause\\na loss of accuracy for shallow models. As for general mod-\\nels, introducing deep supervision will cause unstable perfor-\\nmance, and the design concept of deep supervision can only\\nbring gains in extremely deep models. The proposed PGI\\ncan effectively handle problems such as information bottle-\\nneck and information broken, and can comprehensively im-\\nprove the accuracy of models of different sizes. The concept\\nof PGI brings two valuable contributions. The first one is to\\nmake the auxiliary supervision method applicable to shal-\\nlow models, while the second one is to make the deep model\\ntraining process obtain more reliable gradients. These gra-\\ndients enable deep models to use more accurate information\\nto establish correct correlations between data and targets.Table 5. Ablation study on PGI.\\nModel APval\\n50:95APval\\n50APval\\n75\\nGELAN-S 46.7% 63.0% 50.7%\\n+ DS 46.5% -0.2 62.9% -0.1 50.5% -0.2\\n+ PGI 46.8% +0.1 63.4% +0.4 50.7% =\\nGELAN-M 51.1% 67.9% 55.7%\\n+ DS 51.2% +0.1 68.2% +0.3 55.7% =\\n+ PGI 51.4% +0.3 68.1% +0.2 56.1% +0.4\\nGELAN-C 52.5% 69.5% 57.3%\\n+ DS 52.5% = 69.9% +0.4 57.1% -0.2\\n+ PGI 53.0% +0.5 70.3% +0.8 57.8% +0.5\\nGELAN-E 55.0% 71.9% 60.0%\\n+ DS 55.3% +0.3 72.3% +0.4 60.2% +0.2\\n+ PGI 55.6% +0.6 72.8% +0.9 60.6% +0.6\\n1DS indicates deep supervision.\\n2-{S, M, C, E }indicate small, medium, compact, and extended models.\\nFinally, we show in the table the results of gradually in-\\ncreasing components from baseline YOLOv7 to YOLOv9-\\nE. The GELAN and PGI we proposed have brought all-\\nround improvement to the model.\\nTable 6. Ablation study on GELAN and PGI.\\nModel #Param. FLOPs APval\\n50:95APval\\nSAPval\\nMAPval\\nL\\nYOLOv7 [63] 36.9 104.7 51.2% 31.8% 55.5% 65.0%\\n+ AF [63] 43.6 130.5 53.0% 35.8% 58.7% 68.9%\\n+ GELAN 41.2 126.4 53.2% 36.2% 58.5% 69.9%\\n+ DHLC [34] 57.3 189.0 55.0% 38.0% 60.6% 70.9%\\n+ PGI 57.3 189.0 55.6% 40.2% 61.0% 71.4%\\n5.5. Visualization\\nThis section will explore the information bottleneck is-\\nsues and visualize them. In addition, we will also visualize\\nhow the proposed PGI uses reliable gradients to find the\\ncorrect correlations between data and targets. In Figure 6\\nwe show the visualization results of feature maps obtained\\nby using random initial weights as feedforward under dif-\\nferent architectures. We can see that as the number of lay-\\ners increases, the original information of all architectures\\ngradually decreases. For example, at the 50thlayer of the\\nPlainNet, it is difficult to see the location of objects, and all\\ndistinguishable features will be lost at the 100thlayer. As\\nfor ResNet, although the position of object can still be seen\\nat the 50thlayer, the boundary information has been lost.\\nWhen the depth reached to the 100thlayer, the whole image\\nbecomes blurry. Both CSPNet and the proposed GELAN\\nperform very well, and they both can maintain features that\\nsupport clear identification of objects until the 200thlayer.\\nAmong the comparisons, GELAN has more stable results\\nand clearer boundary information.\\n9\\nFigure 6. Feature maps (visualization results) output by random initial weights of PlainNet, ResNet, CSPNet, and GELAN at different\\ndepths. After 100 layers, ResNet begins to produce feedforward output that is enough to obfuscate object information. Our proposed\\nGELAN can still retain quite complete information up to the 150thlayer, and is still sufficiently discriminative up to the 200thlayer.\\nFigure 7. PAN feature maps (visualization results) of GELAN\\nand YOLOv9 (GELAN + PGI) after one epoch of bias warm-up.\\nGELAN originally had some divergence, but after adding PGI’s\\nreversible branch, it is more capable of focusing on the target ob-\\nject.\\nFigure 7 is used to show whether PGI can provide more\\nreliable gradients during the training process, so that the\\nparameters used for updating can effectively capture the\\nrelationship between the input data and the target. Fig-\\nure 7 shows the visualization results of the feature map of\\nGELAN and YOLOv9 (GELAN + PGI) in PAN bias warm-\\nup. From the comparison of Figure 7(b) and (c), we can\\nclearly see that PGI accurately and concisely captures the\\narea containing objects. As for GELAN that does not use\\nPGI, we found that it had divergence when detecting ob-ject boundaries, and it also produced unexpected responses\\nin some background areas. This experiment confirms that\\nPGI can indeed provide better gradients to update parame-\\nters and enable the feedforward stage of the main branch to\\nretain more important features.\\n6. Conclusions\\nIn this paper, we propose to use PGI to solve the infor-\\nmation bottleneck problem and the problem that the deep\\nsupervision mechanism is not suitable for lightweight neu-\\nral networks. We designed GELAN, a highly efficient\\nand lightweight neural network. In terms of object detec-\\ntion, GELAN has strong and stable performance at different\\ncomputational blocks and depth settings. It can indeed be\\nwidely expanded into a model suitable for various inference\\ndevices. For the above two issues, the introduction of PGI\\nallows both lightweight models and deep models to achieve\\nsignificant improvements in accuracy. The YOLOv9, de-\\nsigned by combining PGI and GELAN, has shown strong\\ncompetitiveness. Its excellent design allows the deep model\\nto reduce the number of parameters by 49% and the amount\\nof calculations by 43% compared with YOLOv8, but it still\\nhas a 0.6% AP improvement on MS COCO dataset.\\n7. Acknowledgements\\nThe authors wish to thank National Center for High-\\nperformance Computing (NCHC) for providing computa-\\ntional and storage resources.\\n10\\nReferences\\n[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT:\\nBERT pre-training of image transformers. In International\\nConference on Learning Representations (ICLR) , 2022. 2\\n[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\\nYuan Mark Liao. YOLOv4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934 , 2020. 3\\n[3] Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiang-\\nwen Kong, Jun Li, and Xiangyu Zhang. Reversible column\\nnetworks. In International Conference on Learning Repre-\\nsentations (ICLR) , 2023. 2, 3, 5\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\\nto-end object detection with transformers. In Proceedings\\nof the European Conference on Computer Vision (ECCV) ,\\npages 213–229, 2020. 3\\n[5] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, and\\nJunni Zou. AP-loss for accurate one-stage object detection.\\nIEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence (TPAMI) , 43(11):3782–3798, 2020. 1\\n[6] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang,\\nWenrui Dai, Hongkai Xiong, and Qi Tian. SdAE: Self-\\ndistillated masked autoencoder. In Proceedings of the Euro-\\npean Conference on Computer Vision (ECCV) , pages 108–\\n124, 2022. 2\\n[7] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin\\nHou, and Ming-Ming Cheng. YOLO-MS: rethinking multi-\\nscale representation learning for real-time object detection.\\narXiv preprint arXiv:2308.05480 , 2023. 1, 3, 7, 8\\n[8] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong\\nWang, and Lu Yuan. DaVIT: Dual attention vision trans-\\nformers. In Proceedings of the European Conference on\\nComputer Vision (ECCV) , pages 74–92, 2022. 1\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. In International Con-\\nference on Learning Representations (ICLR) , 2021. 1, 2\\n[10] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,\\nand Weilin Huang. TOOD: Task-aligned one-stage object\\ndetection. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision (ICCV) , pages 3490–3499,\\n2021. 1\\n[11] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu\\nZhang, Ming-Hsuan Yang, and Philip Torr. Res2Net: A\\nnew multi-scale backbone architecture. IEEE Transac-\\ntions on Pattern Analysis and Machine Intelligence (TPAMI) ,\\n43(2):652–662, 2019. 3\\n[12] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian\\nSun. OTA: Optimal transport assignment for object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 303–\\n312, 2021. 1\\n[13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. YOLOX: Exceeding YOLO series in 2021. arXiv\\npreprint arXiv:2107.08430 , 2021. 3[14] Jocher Glenn. YOLOv5 release v7.0. https://github.\\ncom/ultralytics/yolov5/releases/tag/v7.\\n0, 2022. 3, 7\\n[15] Jocher Glenn. YOLOv8 release v8.1.0. https :\\n/ / github . com / ultralytics / ultralytics /\\nreleases/tag/v8.1.0 , 2024. 3, 7\\n[16] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B\\nGrosse. The reversible residual network: Backpropagation\\nwithout storing activations. Advances in Neural Information\\nProcessing Systems (NeurIPS) , 2017. 2, 3\\n[17] Albert Gu and Tri Dao. Mamba: Linear-time sequence\\nmodeling with selective state spaces. arXiv preprint\\narXiv:2312.00752 , 2023. 1\\n[18] Chaoxu Guo, Bin Fan, Qian Zhang, Shiming Xiang, and\\nChunhong Pan. AugFPN: Improving multi-scale fea-\\nture learning for object detection. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 12595–12604, 2020. 1, 3\\n[19] Qi Han, Yuxuan Cai, and Xiangyu Zhang. RevColV2: Ex-\\nploring disentangled representations in masked image mod-\\neling. Advances in Neural Information Processing Systems\\n(NeurIPS) , 2023. 2, 3\\n[20] Zeeshan Hayder, Xuming He, and Mathieu Salzmann.\\nBoundary-aware instance segmentation. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 5696–5704, 2017. 1, 3\\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 770–778, 2016. 1, 4, 8\\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nIdentity mappings in deep residual networks. In Proceedings\\nof the European Conference on Computer Vision (ECCV) ,\\npages 630–645. Springer, 2016. 1, 4\\n[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger. Densely connected convolutional net-\\nworks. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n4700–4708, 2017. 1\\n[24] Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, and Win-\\nston H Hsu. MonoDTR: Monocular 3D object detection with\\ndepth-aware transformer. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 4012–4021, 2022. 1, 3\\n[25] Lin Huang, Weisheng Li, Linlin Shen, Haojie Fu, Xue Xiao,\\nand Suihan Xiao. YOLOCS: Object detection based on dense\\nchannel compression for feature spatial solidification. arXiv\\npreprint arXiv:2305.04170 , 2023. 3\\n[26] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,\\nAndrew Zisserman, and Joao Carreira. Perceiver: General\\nperception with iterative attention. In International Confer-\\nence on Machine Learning (ICML) , pages 4651–4664, 2021.\\n1\\n[27] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina\\nToutanova. BERT: Pre-training of deep bidirectional trans-\\nformers for language understanding. In Proceedings of\\nNAACL-HLT , volume 1, page 2, 2019. 2\\n11\\n[28] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou\\nZhang, and Zhuowen Tu. Deeply-supervised nets. In Ar-\\ntificial Intelligence and Statistics , pages 562–570, 2015. 1,\\n2, 3\\n[29] Alex Levinshtein, Alborz Rezazadeh Sereshkeh, and Kon-\\nstantinos Derpanis. DATNet: Dense auxiliary tasks for ob-\\nject detection. In Proceedings of the IEEE/CVF Winter Con-\\nference on Applications of Computer Vision (WACV) , pages\\n1419–1427, 2020. 1, 3\\n[30] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng\\nCheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangx-\\niang Chu. YOLOv6 v3.0: A full-scale reloading. arXiv\\npreprint arXiv:2301.05586 , 2023. 3, 7, 2, 4\\n[31] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei\\nGeng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\\nWeiqiang Nie, et al. YOLOv6: A single-stage object de-\\ntection framework for industrial applications. arXiv preprint\\narXiv:2209.02976 , 2022. 3\\n[32] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng\\nLi, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang,\\nWenhai Wang, et al. Uni-perceiver v2: A generalist model\\nfor large-scale vision and vision-language tasks. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 2691–2700, 2023. 1\\n[33] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. A\\ndual weighting label assignment scheme for object detection.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 9387–9396,\\n2022. 1\\n[34] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,\\nZhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CB-\\nNet: A composite backbone network architecture for object\\ndetection. IEEE Transactions on Image Processing (TIP) ,\\n2022. 3, 9\\n[35] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie. Feature pyra-\\nmid networks for object detection. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 2117–2125, 2017. 3\\n[36] Zhihao Lin, Yongtao Wang, Jinhe Zhang, and Xiaojie Chu.\\nDynamicDet: A unified dynamic architecture for object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n6282–6291, 2023. 3, 6, 2, 4\\n[37] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\\nPath aggregation network for instance segmentation. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR) , pages 8759–8768, 2018.\\n3, 5\\n[38] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi\\nXie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba:\\nVisual state space model. arXiv preprint arXiv:2401.10166 ,\\n2024. 1\\n[39] Yudong Liu, Yongtao Wang, Siwei Wang, TingTing Liang,\\nQijie Zhao, Zhi Tang, and Haibin Ling. CBNet: A novel\\ncomposite backbone network architecture for object detec-\\ntion. In Proceedings of the AAAI Conference on Artificial\\nIntelligence (AAAI) , pages 11653–11660, 2020. 3[40] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\\nSwin transformer v2: Scaling up capacity and resolution. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR) , 2022. 1\\n[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV) , pages 10012–10022, 2021. 1\\n[42] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\\nenhofer, Trevor Darrell, and Saining Xie. A ConvNet for the\\n2020s. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 11976–\\n11986, 2022. 1\\n[43] Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang,\\nJinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, and\\nYi Liu. DETRs beat YOLOs on real-time object detection.\\narXiv preprint arXiv:2304.08069 , 2023. 1, 3, 7, 8, 2, 4\\n[44] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou,\\nYudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen.\\nRTMDet: An empirical study of designing real-time object\\ndetectors. arXiv preprint arXiv:2212.07784 , 2022. 8, 2, 3, 4\\n[45] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. A ranking-based, balanced loss function unify-\\ning classification and localisation in object detection. Ad-\\nvances in Neural Information Processing Systems (NeurIPS) ,\\n33:15534–15545, 2020. 1\\n[46] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. Rank & sort loss for object detection and instance\\nsegmentation. In Proceedings of the IEEE/CVF Interna-\\ntional Conference on Computer Vision (ICCV) , pages 3009–\\n3018, 2021. 1\\n[47] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\\nFarhadi. You only look once: Unified, real-time object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 779–\\n788, 2016. 3\\n[48] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\\nstronger. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7263–7271, 2017. 3\\n[49] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental\\nimprovement. arXiv preprint arXiv:1804.02767 , 2018. 3, 8\\n[50] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir\\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\\ntersection over union: A metric and a loss for bounding box\\nregression. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n658–666, 2019. 1\\n[51] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,\\nYurong Chen, and Xiangyang Xue. Object detection from\\nscratch with deep supervision. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence (TPAMI) , 42(2):398–412,\\n2019. 1, 2\\n[52] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-\\nactor: A multi-task transformer for robotic manipulation.\\n12\\nInConference on Robot Learning (CoRL) , pages 785–799,\\n2023. 1\\n[53] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan,\\nChanghu Wang, and Ping Luo. What makes for end-to-end\\nobject detection? In International Conference on Machine\\nLearning (ICML) , pages 9934–9944, 2021. 3\\n[54] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich. Going deeper with\\nconvolutions. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n1–9, 2015. 1, 2, 3\\n[55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\\ntecture for computer vision. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 2818–2826, 2016. 1\\n[56] Zineng Tang, Jaemin Cho, Jie Lei, and Mohit Bansal.\\nPerceiver-VL: Efficient vision-and-language modeling with\\niterative latent attention. In Proceedings of the IEEE/CVF\\nWinter Conference on Applications of Computer Vision\\n(WACV) , pages 4410–4420, 2023. 1\\n[57] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nFully convolutional one-stage object detection. In Proceed-\\nings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV) , pages 9627–9636, 2019. 3\\n[58] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nA simple and strong anchor-free object detector. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(TPAMI) , 44(4):1922–1933, 2022. 3\\n[59] Naftali Tishby and Noga Zaslavsky. Deep learning and the\\ninformation bottleneck principle. In IEEE Information The-\\nory Workshop (ITW) , pages 1–5, 2015. 2, 4\\n[60] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,\\nPeyman Milanfar, Alan Bovik, and Yinxiao Li. MaxVIT:\\nMulti-axis vision transformer. In Proceedings of the Euro-\\npean Conference on Computer Vision (ECCV) , pages 459–\\n479, 2022. 1\\n[61] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo,\\nChuanjian Liu, Kai Han, and Yunhe Wang. Gold-YOLO:\\nEfficient object detector via gather-and-distribute mecha-\\nnism. Advances in Neural Information Processing Systems\\n(NeurIPS) , 2023. 3, 7, 2, 4\\n[62] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. Scaled-YOLOv4: Scaling cross stage\\npartial network. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 13029–13038, 2021. 3\\n[63] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. YOLOv7: Trainable bag-of-freebies\\nsets new state-of-the-art for real-time object detectors. In\\nProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 7464–7475,\\n2023. 3, 6, 7, 9, 1\\n[64] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSPNet: A\\nnew backbone that can enhance learning capability of CNN.InProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition Workshops (CVPRW) , pages\\n390–391, 2020. 3, 6, 8\\n[65] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh.\\nDesigning network design strategies through gradient path\\nanalysis. Journal of Information Science and Engineering\\n(JISE) , 39(4):975–995, 2023. 2, 3, 6\\n[66] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.\\nYou only learn one representation: Unified network for mul-\\ntiple tasks. Journal of Information Science & Engineering\\n(JISE) , 39(3):691–709, 2023. 2, 3, 4\\n[67] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian\\nSun, and Nanning Zheng. End-to-end object detection\\nwith fully convolutional network. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 15849–15858, 2021. 1, 3\\n[68] Liwei Wang, Chen-Yu Lee, Zhuowen Tu, and Svetlana\\nLazebnik. Training deeper convolutional networks with deep\\nsupervision. arXiv preprint arXiv:1505.02496 , 2015. 1, 2, 3\\n[69] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\\nPyramid vision transformer: A versatile backbone for dense\\nprediction without convolutions. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision\\n(ICCV) , pages 568–578, 2021. 1\\n[70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. PVT\\nv2: Improved baselines with pyramid vision transformer.\\nComputational Visual Media , 8(3):415–424, 2022. 1\\n[71] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei\\nChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-\\nvNeXt v2: Co-designing and scaling convnets with masked\\nautoencoders. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n16133–16142, 2023. 1, 2\\n[72] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and\\nKaiming He. Aggregated residual transformations for deep\\nneural networks. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 1492–1500, 2017. 1\\n[73] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\\nBao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: A simple\\nframework for masked image modeling. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 9653–9663, 2022. 2\\n[74] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang,\\nCheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing\\nDang, Shengyu Wei, Yuning Du, et al. PP-YOLOE: An\\nevolved version of YOLO. arXiv preprint arXiv:2203.16250 ,\\n2022. 3, 8, 2, 4\\n[75] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang,\\nYuan Zhang, and Xiuyu Sun. DAMO-YOLO: A re-\\nport on real-time object detection design. arXiv preprint\\narXiv:2211.15444 , 2022. 3, 7, 2, 4\\n[76] Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Ziteng Cui, Yu\\nQiao, Hongsheng Li, and Peng Gao. MonoDETR: Depth-\\nguided transformer for monocular 3D object detection. In\\n13\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV) , pages 9155–9166, 2023. 1, 3\\n[77] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang\\nYe, and Dongwei Ren. Distance-IoU loss: Faster and bet-\\nter learning for bounding box regression. In Proceedings of\\nthe AAAI Conference on Artificial Intelligence (AAAI) , vol-\\nume 34, pages 12993–13000, 2020. 1\\n[78] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo\\nYin, Yuchao Dai, and Ruigang Yang. IoU loss for 2D/3D\\nobject detection. In International Conference on 3D Vision\\n(3DV) , pages 85–94, 2019. 1\\n[79] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,\\nSongtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differ-\\nentiable label assignment for dense object detection. arXiv\\npreprint arXiv:2007.03496 , 2020. 1\\n[80] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,\\nWenyu Liu, and Xinggang Wang. Vision mamba: Efficient\\nvisual representation learning with bidirectional state space\\nmodel. arXiv preprint arXiv:2401.09417 , 2024. 1\\n[81] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng\\nLi, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pre-\\ntraining unified architecture for generic perception for zero-\\nshot and few-shot tasks. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 16804–16815, 2022. 1\\n[82] Zhuofan Zong, Guanglu Song, and Yu Liu. DETRs with\\ncollaborative hybrid assignments training. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 6748–6758, 2023. 3\\n14\\nAppendix\\nA. Implementation Details\\nTable 1. Hyper parameter settings of YOLOv9.\\nhyper parameter value\\nepochs 500\\noptimizer SGD\\ninitial learning rate 0.01\\nfinish learning rate 0.0001\\nlearning rate decay linear\\nmomentum 0.937\\nweight decay 0.0005\\nwarm-up epochs 3\\nwarm-up momentum 0.8\\nwarm-up bias learning rate 0.1\\nbox loss gain 7.5\\nclass loss gain 0.5\\nDFL loss gain 1.5\\nHSV saturation augmentation 0.7\\nHSV value augmentation 0.4\\ntranslation augmentation 0.1\\nscale augmentation 0.9\\nmosaic augmentation 1.0\\nMixUp augmentation 0.15\\ncopy & paste augmentation 0.3\\nclose mosaic epochs 15\\nThe training parameters of YOLOv9 are shown in Ta-\\nble 1. We fully follow the settings of YOLOv7 AF [63],\\nwhich is to use SGD optimizer to train 500 epochs. We first\\nwarm-up for 3 epochs and only update the bias during the\\nwarm-up stage. Next we step down from the initial learning\\nrate 0.01 to 0.0001 in linear decay manner, and the data aug-\\nmentation settings are listed in the bottom part of Table 1.\\nWe shut down mosaic data augmentation operations on the\\nlast 15 epochs.Table 2. Network configurations of YOLOv9.\\nIndex Module Route Filters Depth Size Stride\\n0 Conv – 64 – 3 2\\n1 Conv 0 128 – 3 2\\n2 CSP-ELAN 1 256, 128, 64 2, 1 – 1\\n3 DOWN 2 256 – 3 2\\n4 CSP-ELAN 3 512, 256, 128 2, 1 – 1\\n5 DOWN 4 512 – 3 2\\n6 CSP-ELAN 5 512, 512, 256 2, 1 – 1\\n7 DOWN 6 512 – 3 2\\n8 CSP-ELAN 7 512, 512, 256 2, 1 – 1\\n9 SPP-ELAN 8 512, 256, 256 3, 1 – 1\\n10 Up 9 512 – – 2\\n11 Concat 10, 6 1024 – – 1\\n12 CSP-ELAN 11 512, 512, 256 2, 1 – 1\\n13 Up 12 512 – – 2\\n14 Concat 13, 4 1024 – – 1\\n15 CSP-ELAN 14 256, 256, 128 2, 1 – 1\\n16 DOWN 15 256 – 3 2\\n17 Concat 16, 12 768 – – 1\\n18 CSP-ELAN 17 512, 512, 256 2, 1 – 1\\n19 DOWN 18 512 – 3 2\\n20 Concat 19, 9 1024 – – 1\\n21 CSP-ELAN 20 512, 512, 256 2, 1 – 1\\n22 Predict 15, 18, 21 – – – –\\nThe network topology of YOLOv9 completely follows\\nYOLOv7 AF [63], that is, we replace ELAN with the pro-\\nposed CSP-ELAN block. As listed in Table 2, the depth\\nparameters of CSP-ELAN are represented as ELAN depth\\nand CSP depth, respectively. As for the parameters of CSP-\\nELAN filters, they are represented as ELAN output fil-\\nter, CSP output filter, and CSP inside filter. In the down-\\nsampling module part, we simplify CSP-DOWN module to\\nDOWN module. DOWN module is composed of a pooling\\nlayer with size 2 and stride 1, and a Conv layer with size 3\\nand stride 2. Finally, we optimized the prediction layer and\\nreplaced top, left, bottom, and right in the regression branch\\nwith decoupled branch.\\n1\\nTable 3. Comparison of state-of-the-art object detectors with different training settings.\\nModel #Param. (M) FLOPs (G) AP 50:95 (%) AP 50(%) AP 75(%) AP S(%) AP M(%) AP L(%)Train-from-scratchDy-YOLOv7 [36] – 181.7 53.9 72.2 58.7 35.3 57.6 66.4\\nDy-YOLOv7-X [36] – 307.9 55.0 73.2 60.0 36.6 58.7 68.5\\nYOLOv9-S (Ours) 7.1 26.4 46.8 63.4 50.7 26.6 56.0 64.5\\nYOLOv9-M (Ours) 20.0 76.3 51.4 68.1 56.1 33.6 57.0 68.0\\nYOLOv9-C (Ours) 25.3 102.1 53.0 70.2 57.8 36.2 58.5 69.3\\nYOLOv9-E (Ours) 34.7 147.1 54.5 71.7 59.2 38.1 59.9 70.3\\nYOLOv9-E (Ours) 44.0 183.9 55.1 72.3 60.7 38.7 60.6 71.4\\nYOLOv9-E (Ours) 57.3 189.0 55.6 72.8 60.6 40.2 61.0 71.4ImageNet PretrainedRTMDet-T [44] 4.8 12.6 41.1 57.9 – – – –\\nRTMDet-S [44] 9.0 25.6 44.6 61.9 – – – –\\nRTMDet-M [44] 24.7 78.6 49.4 66.8 – – – –\\nRTMDet-L [44] 52.3 160.4 51.5 68.8 – – – –\\nRTMDet-X [44] 94.9 283.4 52.8 70.4 – – – –\\nPPYOLOE-S [74] 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\\nPPYOLOE-M [74] 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\\nPPYOLOE-L [74] 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\\nPPYOLOE-X [74] 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\\nRT DETR-L [43] 32 110 53.0 71.6 57.3 34.6 57.3 71.2\\nRT DETR-X [43] 67 234 54.8 73.1 59.4 35.7 59.6 72.9\\nRT DETR-R18 [43] 20 60 46.5 63.8 – – – –\\nRT DETR-R34 [43] 31 92 48.9 66.8 – – – –\\nRT DETR-R50M [43] 36 100 51.3 69.6 – – – –\\nRT DETR-R50 [43] 42 136 53.1 71.3 57.7 34.8 58.0 70.0\\nRT DETR-R101 [43] 76 259 54.3 72.7 58.6 36.0 58.8 72.1\\nGold YOLO-S [61] 21.5 46.0 45.5 62.2 – – – –\\nGold YOLO-M [61] 41.3 57.5 50.2 67.5 – – – –\\nGold YOLO-L [61] 75.1 151.7 52.3 69.6 – – – –Knowledge DistillationYOLOv6-N v3.0 [30] 4.7 11.4 37.5 53.1 – – – –\\nYOLOv6-S v3.0 [30] 18.5 45.3 45.0 61.8 – – – –\\nYOLOv6-M v3.0 [30] 34.9 85.8 50.0 66.9 – – – –\\nYOLOv6-L v3.0 [30] 59.6 150.7 52.8 70.3 – – – –\\nDAMO YOLO-T [75] 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\\nDAMO YOLO-S [75] 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\\nDAMO YOLO-M [75] 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\\nDAMO YOLO-L [75] 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\\nGold YOLO-N [61] 5.6 12.1 39.9 55.9 – – – –\\nGold YOLO-S [61] 21.5 46.0 46.1 63.3 – – – –\\nGold YOLO-M [61] 41.3 57.5 50.9 68.2 – – – –\\nGold YOLO-L [61] 75.1 151.7 53.2 70.5 – – – –Complex SettingGold YOLO-S [61] 21.5 46.0 46.4 63.4 – – – –\\nGold YOLO-M [61] 41.3 57.5 51.1 68.5 – – – –\\nGold YOLO-L [61] 75.1 151.7 53.3 70.9 – – – –\\nYOLOR-CSP [66] 52.9 120.4 52.8 71.2 57.6 – – –\\nYOLOR-CSP-X [66] 96.9 226.8 54.8 73.1 59.7 – – –\\nPPYOLOE+-S [74] 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\\nPPYOLOE+-M [74] 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\\nPPYOLOE+-L [74] 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\\nPPYOLOE+-X [74] 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\\nB. More Comparison\\nWe compare YOLOv9 to state-of-the-art real-time object\\ndetectors trained with different methods. It mainly includes\\nfour different training methods: (1) train-from-scratch: we\\nhave completed most of the comparisons in the text. Here\\nare only list of additional data of DynamicDet [36] for com-\\nparisons; (2) Pretrained by ImageNet: this includes two\\nmethods of using ImageNet for supervised pretrain and self-\\nsupervised pretrain; (3) knowledge distillation: a method\\nto perform additional self-distillation after training is com-pleted; and (4) a more complex training process: a combi-\\nnation of steps including pretrained by ImageNet, knowl-\\nedge distillation, DAMO-YOLO and even additional pre-\\ntrained large object detection dataset. We show the results\\nin Table 3. From this table, we can see that our proposed\\nYOLOv9 performed better than all other methods. Com-\\npared with PPYOLOE+-X trained using ImageNet and Ob-\\njects365, our method still reduces the number of parame-\\nters by 55% and the amount of computation by 11%, and\\nimproving 0.4% AP.\\n2\\nTable 4. Comparison of state-of-the-art object detectors with different training settings (sorted by number of parameters).\\nModel #Param. (M) FLOPs (G) APval\\n50:95(%) APval\\n50(%) APval\\n75(%) APval\\nS(%) APval\\nM(%) APval\\nL(%)\\nYOLOv6-N v3.0 [30] (D) 4.7 11.4 37.5 53.1 – – – –\\nRTMDet-T [44] (I) 4.8 12.6 41.1 57.9 – – – –\\nGold YOLO-N [61] (D) 5.6 12.1 39.9 55.9 – – – –\\nYOLOv9-S (S) 7.1 26.4 46.8 63.4 50.7 26.6 56.0 64.5\\nPPYOLOE+-S [74] (C) 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\\nPPYOLOE-S [74] (I) 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\\nDAMO YOLO-T [75] (D) 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\\nRTMDet-S [44] (I) 9.0 25.6 44.6 61.9 – – – –\\nDAMO YOLO-S [75] (D) 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\\nYOLOv6-S v3.0 [30] (D) 18.5 45.3 45.0 61.8 – – – –\\nRT DETR-R18 [43] (I) 20 60 46.5 63.8 – – – –\\nYOLOv9-M (S) 20.0 76.3 51.4 68.1 56.1 33.6 57.0 68.0\\nGold YOLO-S [61] (C) 21.5 46.0 46.4 63.4 – – – –\\nGold YOLO-S [61] (D) 21.5 46.0 46.1 63.3 – – – –\\nGold YOLO-S [61] (I) 21.5 46.0 45.5 62.2 – – – –\\nPPYOLOE+-M [74] (C) 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\\nPPYOLOE-M [74] (I) 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\\nRTMDet-M [44] (I) 24.7 78.6 49.4 66.8 – – – –\\nYOLOv9-C (S) 25.3 102.1 53.0 70.2 57.8 36.2 58.5 69.3\\nDAMO YOLO-M [75] (D) 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\\nRT DETR-R34 [43] (I) 31 92 48.9 66.8 – – – –\\nRT DETR-L [43] (I) 32 110 53.0 71.6 57.3 34.6 57.3 71.2\\nYOLOv9-E (S) 34.7 147.1 54.5 71.7 59.2 38.1 59.9 70.3\\nYOLOv6-M v3.0 [30] (D) 34.9 85.8 50.0 66.9 – – – –\\nRT DETR-R50M [43] (I) 36 100 51.3 69.6 – – – –\\nGold YOLO-M [61] (C) 41.3 57.5 51.1 68.5 – – – –\\nGold YOLO-M [61] (D) 41.3 57.5 50.9 68.2 – – – –\\nGold YOLO-M [61] (I) 41.3 57.5 50.2 67.5 – – – –\\nRT DETR-R50 [43] (I) 42 136 53.1 71.3 57.7 34.8 58.0 70.0\\nDAMO YOLO-L [75] (D) 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\\nYOLOv9-E (S) 44.0 183.9 55.1 72.3 60.7 38.7 60.6 71.4\\nPPYOLOE+-L [74] (C) 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\\nPPYOLOE-L [74] (I) 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\\nRTMDet-L [44] (I) 52.3 160.4 51.5 68.8 – – – –\\nYOLOR-CSP [66] (C) 52.9 120.4 52.8 71.2 57.6 – – –\\nYOLOv9-E (S) 57.3 189.0 55.6 72.8 60.6 40.2 61.0 71.4\\nYOLOv6-L v3.0 [30] (D) 59.6 150.7 52.8 70.3 – – – –\\nRT DETR-X [43] (I) 67 234 54.8 73.1 59.4 35.7 59.6 72.9\\nGold YOLO-L [61] (C) 75.1 151.7 53.3 70.9 – – – –\\nGold YOLO-L [61] (D) 75.1 151.7 53.2 70.5 – – – –\\nGold YOLO-L [61] (I) 75.1 151.7 52.3 69.6 – – – –\\nRT DETR-R101 [43] (I) 76 259 54.3 72.7 58.6 36.0 58.8 72.1\\nRTMDet-X [44] (I) 94.9 283.4 52.8 70.4 – – – –\\nYOLOR-CSP-X [66] (C) 96.9 226.8 54.8 73.1 59.7 – – –\\nPPYOLOE+-X [74] (C) 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\\nPPYOLOE-X [74] (I) 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\\n1(S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.\\nTable 4 shows the performance of all models sorted by\\nparameter size. Our proposed YOLOv9 is Pareto optimal\\nin all models of different sizes. Among them, we found no\\nother method for Pareto optimal in models with more than\\n20M parameters. The above experimental data shows that\\nour YOLOv9 has excellent parameter usage efficiency.Shown in Table 5 is the performance of all participat-\\ning models sorted by the amount of computation. Our pro-\\nposed YOLOv9 is Pareto optimal in all models with differ-\\nent scales. Among models with more than 60 GFLOPs, only\\nELAN-based DAMO-YOLO and DETR-based RT DETR\\ncan rival the proposed YOLOv9. The above comparison\\nresults show that YOLOv9 has the most outstanding per-\\nformance in the trade-off between computation complexity\\nand accuracy.\\n3\\nTable 5. Comparison of state-of-the-art object detectors with different training settings (sorted by amount of computation).\\nModel #Param. (M) FLOPs (G) APval\\n50:95(%) APval\\n50(%) APval\\n75(%) APval\\nS(%) APval\\nM(%) APval\\nL(%)\\nYOLOv6-N v3.0 [30] (D) 4.7 11.4 37.5 53.1 – – – –\\nGold YOLO-N [61] (D) 5.6 12.1 39.9 55.9 – – – –\\nRTMDet-T [44] (I) 4.8 12.6 41.1 57.9 – – – –\\nPPYOLOE+-S [74] (C) 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\\nPPYOLOE-S [74] (I) 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\\nDAMO YOLO-T [75] (D) 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\\nRTMDet-S [44] (I) 9.0 25.6 44.6 61.9 – – – –\\nYOLOv9-S (S) 7.1 26.4 46.8 63.4 50.7 26.6 56.0 64.5\\nDAMO YOLO-S [75] (D) 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\\nYOLOv6-S v3.0 [30] (D) 18.5 45.3 45.0 61.8 – – – –\\nGold YOLO-S [61] (C) 21.5 46.0 46.4 63.4 – – – –\\nGold YOLO-S [61] (D) 21.5 46.0 46.1 63.3 – – – –\\nGold YOLO-S [61] (I) 21.5 46.0 45.5 62.2 – – – –\\nPPYOLOE+-M [74] (C) 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\\nPPYOLOE-M [74] (I) 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\\nGold YOLO-M [61] (C) 41.3 57.5 51.1 68.5 – – – –\\nGold YOLO-M [61] (D) 41.3 57.5 50.9 68.2 – – – –\\nGold YOLO-M [61] (I) 41.3 57.5 50.2 67.5 – – – –\\nRT DETR-R18 [43] (I) 20 60 46.5 63.8 – – – –\\nDAMO YOLO-M [75] (D) 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\\nYOLOv9-M (S) 20.0 76.3 51.4 68.1 56.1 33.6 57.0 68.0\\nRTMDet-M [44] (I) 24.7 78.6 49.4 66.8 – – – –\\nYOLOv6-M v3.0 [30] (D) 34.9 85.8 50.0 66.9 – – – –\\nRT DETR-R34 [43] (I) 31 92 48.9 66.8 – – – –\\nDAMO YOLO-L [75] (D) 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\\nRT DETR-R50M [43] (I) 36 100 51.3 69.6 – – – –\\nYOLOv9-C (S) 25.3 102.1 53.0 70.2 57.8 36.2 58.5 69.3\\nRT DETR-L [43] (I) 32 110 53.0 71.6 57.3 34.6 57.3 71.2\\nPPYOLOE+-L [74] (C) 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\\nPPYOLOE-L [74] (I) 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\\nYOLOR-CSP [66] (C) 52.9 120.4 52.8 71.2 57.6 – – –\\nRT DETR-R50 [43] (I) 42 136 53.1 71.3 57.7 34.8 58.0 70.0\\nYOLOv9-E (S) 34.7 147.1 54.5 71.7 59.2 38.1 59.9 70.3\\nYOLOv6-L v3.0 [30] (D) 59.6 150.7 52.8 70.3 – – – –\\nGold YOLO-L [61] (C) 75.1 151.7 53.3 70.9 – – – –\\nGold YOLO-L [61] (D) 75.1 151.7 53.2 70.5 – – – –\\nGold YOLO-L [61] (I) 75.1 151.7 52.3 69.6 – – – –\\nRTMDet-L [44] (I) 52.3 160.4 51.5 68.8 – – – –\\nDy-YOLOv7 [36] (S) – 181.7 53.9 72.2 58.7 35.3 57.6 66.4\\nYOLOv9-E (S) 44.0 183.9 55.1 72.3 60.7 38.7 60.6 71.4\\nYOLOv9-E (S) 57.3 189.0 55.6 72.8 60.6 40.2 61.0 71.4\\nPPYOLOE+-X [74] (C) 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\\nPPYOLOE-X [74] (I) 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\\nYOLOR-CSP-X [66] (C) 96.9 226.8 54.8 73.1 59.7 – – –\\nRT DETR-X [43] (I) 67 234 54.8 73.1 59.4 35.7 59.6 72.9\\nRT DETR-R101 [43] (I) 76 259 54.3 72.7 58.6 36.0 58.8 72.1\\nRTMDet-X [44] (I) 94.9 283.4 52.8 70.4 – – – –\\nDy-YOLOv7-X [36] (S) – 307.9 55.0 73.2 60.0 36.6 58.7 68.5\\n1(S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.\\n4\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trích xuất hình ảnh từ pdf"
      ],
      "metadata": {
        "id": "vICJl691cFSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "reader = PdfReader('yolov9.pdf')\n",
        "\n",
        "count = 0\n",
        "\n",
        "for page in reader.pages:\n",
        "    for image_file_object in page.images:\n",
        "        with open(str(count) + image_file_object.name, 'wb') as fp:\n",
        "            fp.write(image_file_object.data)\n",
        "            count += 1"
      ],
      "metadata": {
        "id": "y10BoNBTb_gi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nối các file pdf"
      ],
      "metadata": {
        "id": "PK0IhNqBchp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!arxiv-downloader 2209.02976\n",
        "!arxiv-downloader 2207.02696"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "becTqmegcuge",
        "outputId": "8198f192-a1e1-4c24-ef41-aabc519bd9a4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting download of article: \"YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications\" (2209.02976)\n",
            "Download finished! Result saved at:\n",
            "./2209.02976v1.YOLOv6__A_Single_Stage_Object_Detection_Framework_for_Industrial_Applications.pdf\n",
            "Starting download of article: \"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors\" (2207.02696)\n",
            "Download finished! Result saved at:\n",
            "./2207.02696v1.YOLOv7__Trainable_bag_of_freebies_sets_new_state_of_the_art_for_real_time_object_detectors.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 2209.02976v1.YOLOv6__A_Single_Stage_Object_Detection_Framework_for_Industrial_Applications.pdf yolov6.pdf\n",
        "!mv 2207.02696v1.YOLOv7__Trainable_bag_of_freebies_sets_new_state_of_the_art_for_real_time_object_detectors.pdf yolov7.pdf"
      ],
      "metadata": {
        "id": "MXUF6NuMc2nI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfWriter\n",
        "\n",
        "merger = PdfWriter()\n",
        "\n",
        "for pdf in ['yolov6.pdf', 'yolov7.pdf', 'yolov9.pdf']:\n",
        "    merger.append(pdf)\n",
        "\n",
        "merger.write('merged-yolov-679.pdf')\n",
        "merger.close"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "RqNxIJyBcZCX",
        "outputId": "fe68fe05-48fc-45da-a56e-485183edce76"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method PdfWriter.close of <pypdf._writer.PdfWriter object at 0x7e1b2f4efe80>>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pypdf._writer.PdfWriter.close</b><br/>def close() -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pypdf/_writer.py</a>To match the functions from Merger.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 2741);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nén file pdf"
      ],
      "metadata": {
        "id": "_QIQYbgfdY9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfWriter\n",
        "\n",
        "writer = PdfWriter(clone_from='yolov9.pdf')\n",
        "\n",
        "for page in writer.pages:\n",
        "    page.compress_content_streams(level=8)\n",
        "\n",
        "with open('out.pdf', 'wb') as f:\n",
        "    writer.write(f)"
      ],
      "metadata": {
        "id": "aeIBKXTbdaw7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_size = os.path.getsize('yolov9.pdf') / (1024 * 1024)\n",
        "\n",
        "print(f'yolov9.pdf size: {file_size}')\n",
        "\n",
        "file_size = os.path.getsize('out.pdf') / (1024 * 1024)\n",
        "\n",
        "print(f'out.pdf size: {file_size}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gZSq5eedtci",
        "outputId": "77740538-0a50-4a4f-96c8-574d2a0b4dff"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolov9.pdf size: 4.738467216491699\n",
            "out.pdf size: 4.718204498291016\n"
          ]
        }
      ]
    }
  ]
}